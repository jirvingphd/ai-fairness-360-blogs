{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blog Post Series - AI Fairness 360: Mitigating Bias in Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: **Creating Dataset Objects in AI Fairness 360 and Exploring Fairness Metrics**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to AI Fairness\n",
    "\n",
    "Artificial Intelligence (AI) has revolutionized various sectors, from healthcare to finance, by providing powerful tools to analyze data and make informed decisions. However, as these AI systems become more integrated into society, ensuring that they operate fairly and without bias becomes increasingly important. Bias in AI can lead to unfair outcomes, which can have significant social and ethical implications.\n",
    "\n",
    "To address this issue, IBM developed AI Fairness 360 (AIF360), an open-source toolkit designed to help developers detect and mitigate bias in machine learning models. \n",
    "\n",
    "In this lesson, we will explore what AI Fairness 360 is, why fairness in AI is crucial, and how to start using the toolkit to create more equitable AI systems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this blog post, you will be able to:\n",
    "- Understand the concept of bias in AI and its implications.\n",
    "- Learn how to use AI Fairness 360 (AIF360) to detect and mitigate bias.\n",
    "- Create dataset objects using AIF360's `BinaryLabelDataset` and `StandardDataset` classes.\n",
    "- Utilize fairness metrics to evaluate bias in datasets and models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before we begin, ensure you have the following installed:\n",
    "- Python 3.x\n",
    "- Pandas\n",
    "- AI Fairness 360\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Understanding Bias in AI\n",
    "\n",
    "As we delve into the topic of fairness in AI, it's essential to first understand what bias is and how it can manifest in machine learning systems.\n",
    "\n",
    "**Types of Bias**\n",
    "Bias in AI can stem from various sources and can be broadly categorized into three types:\n",
    "\n",
    "1. **Data Bias**: This occurs when the training data used to build the model is not representative of the real-world population. For example, if a facial recognition system is trained predominantly on images of lighter-skinned individuals, it may perform poorly on individuals with darker skin tones.\n",
    "\n",
    "2. **Algorithmic Bias**: This arises when the algorithms used in the AI system inadvertently amplify existing biases in the data. Even if the training data is balanced, the way the algorithm processes this data can still introduce bias.\n",
    "\n",
    "3. **Societal Bias**: This type of bias reflects broader societal prejudices and inequalities. AI systems, being products of human society, can inherit and perpetuate these biases if not carefully designed and monitored.\n",
    "\n",
    "**Impact of Bias**\n",
    "The impact of biased AI systems can be profound and far-reaching. Biased models can lead to discriminatory practices in areas such as hiring, lending, law enforcement, and healthcare. For instance, a biased hiring algorithm might unfairly favor candidates from certain demographic groups, leading to unequal employment opportunities.\n",
    "\n",
    "Real-world examples of biased AI systems highlight the urgency of addressing this issue. For instance, a study found that a widely used healthcare algorithm disproportionately favored white patients over black patients when determining eligibility for certain health programs, leading to disparities in care.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to AI Fairness 360\n",
    "\n",
    "AI Fairness 360 (AIF360) is an open-source toolkit developed by IBM to address the challenge of bias in machine learning models. The toolkit provides a comprehensive suite of metrics to test for biases and algorithms to mitigate them.\n",
    "\n",
    "**Toolkit Overview**\n",
    "AIF360 includes:\n",
    "\n",
    "- **Datasets**: A variety of datasets commonly used in fairness research, such as the Adult Income dataset and the COMPAS dataset.\n",
    "- **Metrics**: A collection of fairness metrics to evaluate bias in datasets and models, including disparate impact, statistical parity, and equal opportunity difference.\n",
    "- **Algorithms**: Several bias mitigation algorithms, such as reweighing, prejudice remover, and adversarial debiasing.\n",
    "\n",
    "**Installation**\n",
    "To get started with AI Fairness 360, you first need to install the toolkit. You can install it using pip:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can install the base components AI Fairness 360 using pip:\n",
    "```bash\n",
    "pip install aif360\n",
    "```\n",
    "\n",
    "To install all optional dependecies use:\n",
    "```bash\n",
    "pip install 'aif360[all]'\n",
    "```\n",
    "\n",
    "For more information see https://aif360.readthedocs.io/en/latest/Getting%20Started.html#installation.\n",
    "\n",
    "\n",
    "In the following sections, we will walk through a practical example using AIF360 to detect and mitigate bias in a machine learning model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Practical Example: Detecting and Mitigating Bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this example, we will use the Recidivism for Offenders Released from Prison dataset [directly from the IOWA Open Data Portal](https://data.iowa.gov/Correctional-System/Iowa-Prison-Recidivism-Status/akzb-ddk8/about_data), which provides information on individuals released from prison and their likelihood of reoffending. This dataset is suitable for studying fairness as it contains various demographic features that can be analyzed for bias.\n",
    "\n",
    "\n",
    "**Dataset Construction**\n",
    "To use AIFairness 360, the data must be converted into a proprietary AIF360 dataset object. The requirements for the Dataset objects are as follows:\n",
    "1. Categorical Features encoded numerically (Not One hot encoded).\n",
    "2. No null values.\n",
    "3. Defined **Protected attributes** (e.g. Race, Sex).\n",
    "    - **Definition**: Attributes that refer to characteristics of individuals that are legally protected against discrimination. These can include race, gender, age, religion, disability status, and other similar attributes.\n",
    "4. For each protected attribute, defined Privileged groups (e.g. Male or White) and Unpriviledged Groups (e.g. Female, Non-white). \n",
    "    - **Privileged Groups**: Subsets of individuals within a dataset that have historically been favored or have had advantages in societal contexts based on certain protected attributes.\n",
    "    - **Unprivileged Groups**: Subsets of individuals within a dataset that have historically been disadvantaged or have faced discrimination based on certain protected attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Introduction to AIF360 Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use AIF360, we must convert our DataFrames into one of two classes from AIF360: the `BinaryLabelDataset` and the `StandardDataset`.\n",
    "\n",
    "In this blog post, we will focus on creating dataset objects using both the `BinaryLabelDataset` and `StandardDataset` classes provided by AIF360. \n",
    "\n",
    "Additionally, we will begin to explore some key fairness metrics to understand how bias can be measured and addressed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Loading the Dataset as DataFrame First\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "# Load the data \n",
    "df = pd.read_csv(\"blog_post/data/Iowa_Prison_Recidivism_Status_20240724.csv\", \n",
    "                 index_col=0, usecols=range(0, 23-7))   \n",
    "\n",
    "## Quick Conversion of Dtypes for Clean Data\n",
    "df = df.convert_dtypes(convert_string=False)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "drop_cols = ['Supervising Unit','Supervision Start Date','Supervision End Date'] + [c for c in df.columns if 'Year' in c]\n",
    "df = df.drop(columns=drop_cols)\n",
    "df.info()\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Encoding Protected Attributes Features as Integers\n",
    "\n",
    "To work with AIF360, we need to encode the categorical features numerically. In this example, we'll focus on the 'race' attribute as a protected attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview unique classes for protected attributes\n",
    "df['Race'].unique(), df['Sex'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I have not yet found an official statement on whether protected attribute columns are able to be used with the metrics we will be discussing in this post, this author was unable to get mult-classes protected attributes to work. Therefore, for now, we have converted the Race column into a binary column where 0 represents being White and 1 represents Non-White. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the 'race' column\n",
    "# race_map = {'White': 0, 'Black': 1, 'Hispanic': 2, 'Asian or Pacific Islander': 3,\n",
    "            # 'American Indian or Alaska Native': 4, 'Unknown': 5, 'Other':6}\n",
    "race_map = {'White': 0, 'Black': 1, 'Hispanic': 1, 'Asian or Pacific Islander': 1,\n",
    "            'American Indian or Alaska Native': 1, 'Unknown': 1, 'Other':1}\n",
    "df['Race'] = df['Race'].map(race_map)\n",
    "\n",
    "# Encode the \"Sex\" column\n",
    "sex_map = {\"Male\": 0, \"Female\":1}\n",
    "df['Sex'] = df['Sex'].map(sex_map)\n",
    "\n",
    "df['Race'].unique(), df['Sex'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using both string and integer labels for our groups throughout this example. Therefore, we will save the dictionary maps together for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the protcted attribute mapping dictionaries to a dict\n",
    "protected_attribute_maps = {\"Race\":race_map,\n",
    "                            \"Sex\":sex_map}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for Dataset Object Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AI Fairness 360 (AIF360) toolkit provides different dataset classes to handle various types of data structures and use cases. Two commonly used classes are `BinaryLabelDataset` and `StandardDataset`. \n",
    "\n",
    "Both classes require that:\n",
    "- The protected attributes (i.e. Race and Sex) and target (Reincarcerated) must be encoded as integers.\n",
    "- All features must be numerically encoded (Ohe Hot Encoded categorical features, Boolean features as integers)\n",
    "- No null values are present.\n",
    "\n",
    "Therefore we will prepare the data as if we were preparing it for machine learning. However, we will not produce a train-test-split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute NaNs with most frequent value\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "# Enable pandas dataframe output\n",
    "from sklearn import set_config\n",
    "set_config(transform_output='pandas')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Categorical Pipeline\n",
    "cat_cols = df.select_dtypes(include='object').columns\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "cat_pipe = make_pipeline(cat_imputer, ohe)\n",
    "\n",
    "\n",
    "# Numeric Pipeline\n",
    "num_cols = df.select_dtypes(include='number').columns\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "num_pipe = make_pipeline(num_imputer)\n",
    "\n",
    "\n",
    "# Convert Boolean Columns to Integers\n",
    "bool_cols = df.select_dtypes(include='bool').columns\n",
    "df[bool_cols] = df[bool_cols].astype(int)\n",
    "\n",
    "\n",
    "# Create the column Transformer\n",
    "preprocessor = ColumnTransformer(transformers=[('cat', cat_pipe, cat_cols),\n",
    "                                               ('num', num_pipe, num_cols)],\n",
    "                                 remainder='passthrough',\n",
    "                                 verbose_feature_names_out=False)\n",
    "\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and Transform the data\n",
    "final_df = preprocessor.fit_transform(df)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Protected Attributes and Priviledged vs. Unpriveldged Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can convert our preprocessed data to a BinaryDataset or StandardDataset, we need to take a moment to fully map the protected attributes and for each attribute, we must define priviledged groups and unpriviledged groups.\n",
    "\n",
    "If you use one of the Built-In datasets, it comes with a data dictionary. If we construct a similar data dictionary, we can leverage example code from official AIF360 tutorials to convert our data_dict into the correctly formattd group variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining the Data Dict of the Tutorial Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tutorial: Medical Expendature: https://github.com/Trusted-AI/AIF360/blob/main/examples/tutorial_medical_expenditure.ipynb\n",
    "- See the notebook linked above for instructions on running this example locally. \n",
    "Note that we will only briefly analyze the contents of this tutorial example, so you can safely skip over these steps when working through this example on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tutorial Code source:  https://github.com/Trusted-AI/AIF360/blob/main/examples/tutorial_medical_expenditure.ipynb\n",
    "\n",
    "# Datasets\n",
    "from aif360.datasets import MEPSDataset19\n",
    "from aif360.datasets import MEPSDataset20\n",
    "from aif360.datasets import MEPSDataset21\n",
    "\n",
    "# dataset = MEPSDataset19()#.split([0.5, 0.8], shuffle=True)\n",
    "\n",
    "(dataset_orig_panel19_train,\n",
    " dataset_orig_panel19_val,\n",
    " dataset_orig_panel19_test) = MEPSDataset19().split([0.5, 0.8], shuffle=True)\n",
    "\n",
    "# Numeric index for sensitive/protected attribute\n",
    "sens_ind = 0\n",
    "\n",
    "# Name of the sensitive/protected attribute\n",
    "sens_attr = dataset_orig_panel19_train.protected_attribute_names[sens_ind]\n",
    "\n",
    "# List comp to create a list of dictionaries for unprivileged and privileged groups\n",
    "unprivileged_groups = [{sens_attr: v} for v in\n",
    "                       dataset_orig_panel19_train.unprivileged_protected_attributes[sens_ind]]\n",
    "privileged_groups = [{sens_attr: v} for v in\n",
    "                     dataset_orig_panel19_train.privileged_protected_attributes[sens_ind]]\n",
    "\n",
    "# Preview the group vars\n",
    "print(f\"{unprivileged_groups = }\")\n",
    "print(f\"{privileged_groups = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the dataset objects already had information about the `protected_attribute_names` and the integer values for the encoded protected attributes (`privileged_protected_attributes` `unprivileged_protected_attributes` ).\n",
    "\n",
    "The tutorial used a list comprehension to convert the arrays of group integer values into a list of dictionaries, with the `[{Protected Attribute Name:Integer Value}]`.\n",
    "\n",
    "\n",
    "The tutorial then demonstrates instantiated a  BinaryLabelDatasetMetric with the dataset and the list of priviledged group dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fairnes metrics & Explainers\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.explainers import MetricTextExplainer\n",
    "\n",
    "\n",
    "# Metrics for the tutorial dataset\n",
    "metric_orig_panel19_train = BinaryLabelDatasetMetric(\n",
    "        dataset_orig_panel19_train,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial then instanstiates a MetricTextExplainer to facilitate understanding of the metrics.  The first example metric, which will discuss more in-depth later, is Disparate Impact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Explainer to describe the fairness metrics\n",
    "explainer_orig_panel19_train = MetricTextExplainer(metric_orig_panel19_train)\n",
    "# Displaying disparate_impact\n",
    "print(explainer_orig_panel19_train.disparate_impact())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to begin analyzing the metrics of our Prisoner Recividism dataset, we will first want to declare the protected attributes, and priviledged and unpriviledged group numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mimicking the Tutorial's Data  Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting and Inspecting the Tutorial Data Dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If we convert one of the example datasets to a dataframe using the `.convert_to_dataframe()` method, we will get the dataframe, as well as a data dict that stores the information about the protected attributes and priviledge groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the example dataset to a dataframe and data dictionary \n",
    "example_df, example_data_dict = dataset_orig_panel19_train.convert_to_dataframe()#)\n",
    "# display(example_df.head(3))\n",
    "\n",
    "# Exploring the contents of the data dictionary\n",
    "print(\"Data Dict Keys:\")\n",
    "[print(f\"- {k}\") for k in example_data_dict.keys()];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target label var stored in list of strings - Only 1 target column\n",
    "example_data_dict['label_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of protected attribute column names\n",
    "example_data_dict['protected_attribute_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each protected attribute, the data dictionary has an array of the group values that are priviledged (believed to have an unfair advantage) and unpriviledged. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of privileged values for each protected attribute\n",
    "display(example_data_dict['privileged_protected_attributes'])\n",
    "\n",
    "# List of unprivileged values for each protected attribute\n",
    "display(example_data_dict['unprivileged_protected_attributes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Take note that, in this example, we have only 1 protected attribute. \n",
    "\n",
    "For the priviledged/unpriviledged groups, we have a list, with 1 array for each of our protected attributes, but in this case we only have one so its a list of a single array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating our own data_dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's create a new data dictionary based on our Iowa Prisoner Recidivism dataset. In this dataset, we have more than 1 protected attribute, which increases the complexity of our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all of the representations of the groups need to be converted to lists of integers, we will save our integer-group mapping dictionaries to a final dictionary under the key for the corresodning atribute.\n",
    "\n",
    "We will also save a dictionary with a key for each protected attribute and the **names** of the priviledged groups for each protected attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the protcted attribute mapping dictionaries to a dict\n",
    "\n",
    "# Commented Out: previously saved above\n",
    "# protected_attribute_maps = {\"Race\":race_map,\n",
    "#                             \"Sex\":sex_map}\n",
    "\n",
    "print(\"- protected_attribute_maps:\")\n",
    "display(protected_attribute_maps)\n",
    "\n",
    "# Manually defining the list of privileged group NAMES to a dict\n",
    "prot_attrs_priv_group_names = {'Race':[\"White\"],\n",
    "                               \"Sex\":[\"Male\"]}\n",
    "\n",
    "print(\"- prot_attrs_priv_group_names:\")\n",
    "prot_attrs_priv_group_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review the structure of the example data dictionary before we reproduce it for our prisoner recidivism data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revieing the structure of \n",
    "example_data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data dictionary like the one used in AIF360's medical expenditure example\n",
    "target = 'Reincarcerated'\n",
    "\n",
    "# Saving the feature names as a list\n",
    "feature_names = final_df.drop(columns=target).columns\n",
    "protected_attributes_names = list(protected_attribute_maps.keys())\n",
    "\n",
    "# Starting our own data dictionary\n",
    "DATA_DICT = dict(feature_names=feature_names, # list of feature columns\n",
    "                 label_names = [target], # list with just the target label column\n",
    "                 protected_attributes_names = protected_attributes_names, # list of protected attribute columns\n",
    "                 \n",
    "                # Saving information about each row (not used )\n",
    "                #  instance_names = final_df.index, # list of each row's name\n",
    "                #  instance_weights = np.ones_like(final_df.index), # list of each row's weight (1.0 for now\n",
    "                \n",
    "                \n",
    "                # List of arrays of priviledged group numbers: will be filled in later\n",
    "                privileged_protected_attributes = [], \n",
    "                # List of arrays of unpriviledged group numers: Will be filled in later\n",
    "                unprivileged_protected_attributes = [], \n",
    ")\n",
    "                 \n",
    "DATA_DICT.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Loop: Accounting for More than 1 Protected Attribute per Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to produce the group information for every protected attribute in the dataset. In this case, the proteted attributes are Race and Sex.\n",
    "Before we construct a loop to handle both protected attributes, let's manually slice out the first attribute and figure out our workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric index of current protected attribute\n",
    "attr_idx = 0\n",
    "\n",
    "# slice the name of the current protected attribute\n",
    "attr_name = protected_attributes_names[attr_idx]\n",
    "\n",
    "# Get the mapping for the current protected attribute\n",
    "attr_map = protected_attribute_maps[attr_name]\n",
    "attr_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the privileged group names for the current protected attribute\n",
    "priv_group_names = prot_attrs_priv_group_names[attr_name]\n",
    "priv_group_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the privileged values for the current protected attribute\n",
    "priviledged_group_nums = np.array([attr_map[pg] for pg in priv_group_names])\n",
    "priviledged_group_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unprivileged values for the current protected attribute\n",
    "unpriviledged_group_nums = np.array([v for v in attr_map.values() if v not in priviledged_group_nums])\n",
    "unpriviledged_group_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've been able to save the numeric valus for the privileged and unprivileged groups, we can now save them to the data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the privileged and unprivileged group values to the data dictionary\n",
    "DATA_DICT['privileged_protected_attributes'].append(priviledged_group_nums)\n",
    "DATA_DICT['unprivileged_protected_attributes'].append(unpriviledged_group_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get current attributes privileged groups\n",
    "# current_attr_priv_group_names = prot_attrs_priv_group_names[attr_name]\n",
    "# current_attr_priv_group_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save unique groups for the current attribute\n",
    "# unique_groups = current_attr_map_dict.values()\n",
    "# unique_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Data Dictionary Creation Loop: Putting it All Together\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our mapping dictionaries to set the integer values for priviledged and unpriviledged groups. \n",
    "We know for Race, the priviledged group is \"White\" and all other values are unpriviledged groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combining the full workflow into a loop\n",
    "\n",
    "# Creating a data dictionary like the one used in AIF360's medical expenditure example\n",
    "target = 'Reincarcerated'\n",
    "# Save the target label names as a list\n",
    "\n",
    "\n",
    "# Saving the feature names as a list\n",
    "feature_names = final_df.drop(columns=target).columns\n",
    "protected_attributes_names = list(protected_attribute_maps.keys())\n",
    "\n",
    "# Starting our own data dictionary\n",
    "DATA_DICT = dict(feature_names=feature_names, # list of feature columns\n",
    "                 label_names = [target], # list with just the target label column\n",
    "                 protected_attributes_names = protected_attributes_names, # list of protected attribute columns\n",
    "                 \n",
    "                # Saving information about each row (not used )\n",
    "                #  instance_names = final_df.index, # list of each row's name\n",
    "                #  instance_weights = np.ones_like(final_df.index), # list of each row's weight (1.0 for now\n",
    "                \n",
    "                \n",
    "                # List of arrays of priviledged group numbers: will be filled in later\n",
    "                privileged_protected_attributes = [], \n",
    "                # List of arrays of unpriviledged group numers: Will be filled in later\n",
    "                unprivileged_protected_attributes = [], \n",
    ")\n",
    "\n",
    "\n",
    "# Loop through the protected attributes\n",
    "for attr_idx, attr_name in enumerate(protected_attributes_names):\n",
    "    \n",
    "    # Get the mapping for the current protected attribute\n",
    "    attr_map = protected_attribute_maps[attr_name]\n",
    "    \n",
    "    # Get the privileged group names for the current protected attribute\n",
    "    priv_group_names = prot_attrs_priv_group_names[attr_name]\n",
    "    \n",
    "    # Get the privileged values for the current protected attribute\n",
    "    priv_vals = [attr_map[pg] for pg in priv_group_names]\n",
    "    \n",
    "    # Get the unprivileged values for the current protected attribute\n",
    "    unpriv_vals = [v for v in attr_map.values() if v not in priv_vals]\n",
    "    \n",
    "    # Add the privileged and unprivileged values to the data dictionary\n",
    "    DATA_DICT['privileged_protected_attributes'].append(np.array(priv_vals))\n",
    "    DATA_DICT['unprivileged_protected_attributes'].append(np.array(unpriv_vals))\n",
    "    \n",
    "print(f\"{DATA_DICT['privileged_protected_attributes']=}\")\n",
    "print(f\"{DATA_DICT['unprivileged_protected_attributes']=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Our DataFrame to an AIF360 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AI Fairness 360 (AIF360) toolkit provides different dataset classes to handle various types of data structures and use cases. Two commonly used classes are `BinaryLabelDataset` and `StandardDataset`. We will first use a BianryLabelDataset, which was the kind used in the tutorial example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### BinaryLabelDataset\n",
    "\n",
    "**Purpose**: Designed specifically for binary classification tasks where the target variable (label) has only two possible outcomes.\n",
    "\n",
    "**Key Features**:\n",
    "- **Label Handling**: Assumes that the label (target variable) has binary values (e.g., 0 and 1, \"yes\" and \"no\").\n",
    "- **Protected Attributes**: Handles protected attributes for fairness analysis.\n",
    "- **Metrics Compatibility**: Provides compatibility with fairness metrics that are specific to binary classification tasks.\n",
    "- **Examples**: Suitable for datasets like the Prisoner Recidivism dataset, where the task is to predict if the inmate was reincarcerated (binary Yes/No).\n",
    "\n",
    "🔗[Link to Documentation](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.datasets.BinaryLabelDataset.html#aif360-datasets-binarylabeldataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviewing the structure of our DATA_DICT\n",
    "DATA_DICT.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "binary_dataset = BinaryLabelDataset(df=final_df,\n",
    "                                    label_names=DATA_DICT['label_names'],\n",
    "                                    protected_attribute_names=DATA_DICT['protected_attributes_names'],\n",
    "                                    favorable_label = 0, # Non-recividism is favorable\n",
    "                                    unfavorable_label = 1, # Recividism is unfavorable\n",
    "                                    )\n",
    "binary_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Convenience Features of `BinaryLabelDataset`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Simplified Initialization**:\n",
    "   - **Default Configurations**: `BinaryLabelDataset` comes with default configurations optimized for binary classification, making it easier to set up for such tasks without needing extensive customization.\n",
    "\n",
    "2. **Preconfigured Metrics Compatibility**:\n",
    "   - **Metric Calculation**: When using `BinaryLabelDataset`, it’s straightforward to calculate fairness metrics specific to binary classification. These metrics, such as disparate impact or statistical parity difference, are preconfigured for binary outcomes.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For loop (based on tutorial list comp) to create a list of dictionaries \n",
    "# # for unprivileged and privileged groups for each of the protected attributes\n",
    "# unprivileged_groups = []\n",
    "# privileged_groups = []\n",
    "\n",
    "# for sens_ind, sens_attr in enumerate(protected_attributes_names):\n",
    "    \n",
    "#     unprivileged_groups.extend({sens_attr: v} for v in\n",
    "#                        DATA_DICT['unprivileged_protected_attributes'][sens_ind])\n",
    "    \n",
    "#     privileged_groups.extend({sens_attr: v} for v in\n",
    "#                      DATA_DICT['privileged_protected_attributes'][sens_ind])\n",
    "    \n",
    "# unprivileged_groups, privileged_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DICT['unprivileged_protected_attributes'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DICT['protected_attributes_names'] ,DATA_DICT['privileged_protected_attributes'],DATA_DICT['unprivileged_protected_attributes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Fairness 360 Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate fairness metrics, we will leverage 2 classes from AIF360: BinaryLabelDatasetMetric and ClassificationMetric. The BinaryLabelDatasetMetric class is used to calculate the fairness metrics, while the ClassificationMetric class is used to calculate the classification metrics.\n",
    "\n",
    "Additionally, the package includes a text explainer class, MetricTextExplainer, which can be used to explain the fairness metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When instantiateing a meetric class, we must provide the list of privileged groups and unprivileged groups as a list of dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "### TROUBLESHOOTING `BinaryLabelDatasetMetric` CREATION\n",
    "Unsure about parameters `privileged_groups` and `unprivileged_groups`.\n",
    "- If i use the list of numpy arrays from DATA_DICT:\n",
    "    - get error: `AttributeError: 'numpy.ndarray' object has no attribute 'items'`\n",
    "- If i use the list of dictionaries from above (`unprivileged_groups`, `privileged_groups`) \n",
    "    - get error: `ValueError: 'privileged_groups' and 'unprivileged_groups' must be disjoint.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP  testing of args - single dict per attribute\n",
    "\n",
    "# For loop (based on tutorial list comp) to create a list of dictionaries \n",
    "# for unprivileged and privileged groups for each of the protected attributes\n",
    "unprivileged_groups = []\n",
    "privileged_groups = []\n",
    "\n",
    "\n",
    "for sens_ind in range(len(protected_attributes_names)):\n",
    "    # Saving the name of the current protected attribute\n",
    "    sens_attr_name = protected_attributes_names[sens_ind]\n",
    "    \n",
    "    # Save the unique integer values for the current protected attribute\n",
    "    unprivileged_groups.append({sens_attr_name: np.unique(DATA_DICT['unprivileged_protected_attributes'][sens_ind])})\n",
    "    privileged_groups.append({sens_attr_name:  np.unique(DATA_DICT['privileged_protected_attributes'][sens_ind])})\n",
    "\n",
    "unprivileged_groups, privileged_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Using single dict per class (instead of attribut)\n",
    "# unprivileged_groups = []\n",
    "# privileged_groups = []\n",
    "\n",
    "# for send_ind, sens_attr in enumerate(protected_attributes_names):\n",
    "#     unprivileged_groups.extend({sens_attr: v} for v in\n",
    "#                        DATA_DICT['unprivileged_protected_attributes'][sens_ind])\n",
    "    \n",
    "#     privileged_groups.extend({sens_attr: v} for v in\n",
    "#                      DATA_DICT['privileged_protected_attributes'][sens_ind])\n",
    "    \n",
    "# unprivileged_groups, privileged_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BinaryLabelDatasetMetric class must be instantiated for 1 protected attribute at a time (e.g. Sex or Race, but not both).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "\n",
    "# Calculating the disparate impact for Race\n",
    "sens_ind = 0\n",
    "\n",
    "# Printing the name of the sensitive attribute\n",
    "print(f\"{list(unprivileged_groups[sens_ind].keys())[0]}\")\n",
    "\n",
    "metric_race = BinaryLabelDatasetMetric(binary_dataset,\n",
    "                                  privileged_groups=[privileged_groups[sens_ind]],\n",
    "                                  unprivileged_groups=[unprivileged_groups[sens_ind]]\n",
    "                                  )\n",
    "metric_race"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also instantiate a Metric TextExplainer for each class to help us interpret the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a metric text explainer using the BinaryLabelDatasetMetric object\n",
    "explainer_race = MetricTextExplainer(metric_race)\n",
    "explainer_race"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make a second objecct for our second protected attribute: Sex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the disparate impact for sex\n",
    "sens_ind = 1\n",
    "print(f\"{list(unprivileged_groups[sens_ind].keys())[0]}\")\n",
    "\n",
    "# Instantiate the metric class for the protected attribute.\n",
    "metric_sex = BinaryLabelDatasetMetric(binary_dataset,\n",
    "                                  privileged_groups=[privileged_groups[sens_ind]],\n",
    "                                  unprivileged_groups=[unprivileged_groups[sens_ind]]\n",
    "                                  )\n",
    "\n",
    "# Instantiate the explainer for each protected attribute\n",
    "explainer_sex = MetricTextExplainer(metric_sex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness Metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will discuss several AI Fairness 360 metrics in the next section. Some of the metrics include:\n",
    "    - Disparate Impact\n",
    "    - Mean Difference\n",
    "    - Base Rate\n",
    "    - Consistency\n",
    "    - Statistical Parity Difference\n",
    "    - Equal Opportunity Difference\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disparate Impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Disparate impact**, also known as **adverse impact** or **disparate treatment**, is a concept used in the field of employment and civil rights law to assess potential discrimination. It refers to a situation where a particular policy or practice, although neutral on its face, has a disproportionately negative impact on a protected group compared to other groups.\n",
    "\n",
    "- In the context of fairness in machine learning, the disparate impact ratio is a fairness metric that measures the relative likelihood of a favorable outcome between different groups.\n",
    "- It quantifies the difference in the probability of a positive outcome for the privileged group compared to the unprivileged group. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Interpreting Disparate Impact Ratio**\n",
    "-  a disparate impact ratio of 1 indicates no disparity\n",
    "-  a ratio less than 1 indicates a higher likelihood of a positive outcome for the unprivileged group\n",
    "-  a ratio greater than 1 indicates a higher likelihood for the privileged group.\n",
    "\n",
    "Assessing disparate impact is important to identify and address potential biases in machine learning models and ensure fairness in decision-making processes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the disparate impact for sex\n",
    "print(f\"Disparate Impact (Sex): {metric_sex.disparate_impact()}\")\n",
    "\n",
    "\n",
    "# Exlpaining the disparate impact\n",
    "print(explainer_sex.disparate_impact())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of Sex, since the value is greater than 1 (1.126) the dataset is actually biased, with Females being more likely to be non-recidivists than male. We can see this for ourselves by looking at the cross tabulation the Sex and Reincarcerated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(df['Sex'], df['Reincarcerated'],normalize='index')\n",
    "ax = crosstab.plot(kind='bar')# stacked=True)\n",
    "ax.set(ylabel='Proportion of Sex',title='Males (Sex=0) are more likely to be reincarcerated');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the disparate impact for Race\n",
    "print(f\"Disparate Impact (Race): {metric_race.disparate_impact()}\")\n",
    "print(explainer_sex.disparate_impact())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of Race, since the value is greater than 1 (1.05) the dataset is actually biased with non-whites being more likely to be non-recidivists than male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(df['Race'], df['Reincarcerated'],normalize='index')\n",
    "ax = crosstab.plot(kind='bar')# stacked=True)\n",
    "ax.set(ylabel='Proportion of Race',title='White (Race=0) are slighlty more likely to be reincarcerated');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The **mean_difference** fairness metric is a measure used to assess the fairness of a machine learning model's predictions across different groups or demographics. It quantifies the average difference in predicted outcomes between different groups.\n",
    "\n",
    "- In the context of fairness, groups can be defined based on sensitive attributes such as race, gender, or age. \n",
    "- The mean_difference metric calculates the average difference in predicted outcomes (e.g., probabilities or scores) between these groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To calculate the mean_difference fairness metric, you need to follow these steps:\n",
    "\n",
    "1. Identify the sensitive attribute or group that you want to evaluate for fairness.\n",
    "2. Split your dataset into subgroups based on the sensitive attribute.\n",
    "3. For each subgroup, calculate the average predicted outcome.\n",
    "4. Calculate the difference between the average predicted outcomes of different subgroups.\n",
    "5. Take the absolute value of the differences to ensure that negative and positive differences are treated equally.\n",
    "6, Finally, calculate the average of these absolute differences across all subgroups.\n",
    "\n",
    "The resulting value represents the mean_difference fairness metric. \n",
    "\n",
    ">- A value close to zero indicates that the model's predictions are similar across different groups, suggesting fairness. On the other hand, a larger value indicates a significant difference in predictions between groups, suggesting potential bias or unfairness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Difference (Sex): {metric_sex.mean_difference()}\")\n",
    "print(explainer_sex.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate this manually to confirm our understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the mean difference manually\n",
    "group_means = df.groupby(\"Sex\")['Reincarcerated'].mean()\n",
    "group_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract the mean of the unprivileged group from the privileged group\n",
    "abs(group_means.loc[1.0] - group_means.loc[0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the mean difference between groups is ~7.7%, which matches the metric class's results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Difference (Race): {metric_race.mean_difference()}\")\n",
    "print(explainer_race.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the mean difference between groups is ~3.4%.\n",
    "\n",
    "Our dataset is more biased in terms of Sex than it is for Race. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The **base_rate** metric for fairness is a measure that evaluates the proportion of positive outcomes (e.g., approvals, acceptances, etc.) within a particular group. \n",
    "- It is used to compare the rates of favorable outcomes across different groups defined by sensitive attributes such as race, gender, or age.\n",
    "\n",
    "To calculate the base_rate for a group, follow these steps:\n",
    "\n",
    "1. Identify the group based on the sensitive attribute.\n",
    "2. Count the number of positive outcomes (e.g., the number of approved loans) within the group.\n",
    "3. Divide this count by the total number of instances in the group.\n",
    "\n",
    "The formula for the base_rate is:\n",
    "\n",
    "$ \\text{Base Rate} = \\frac{\\text{Number of Positive Outcomes}}{\\text{Total Number of Instances in the Group}} $\n",
    "\n",
    "- The base_rate can then be compared across different groups to assess fairness. If the base_rate is significantly different between groups, it may indicate potential bias or unfairness in the decision-making process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Base Rate\n",
    "# print(explainer_train.base_rate())\n",
    "priv_base_rate = metric_sex.base_rate()\n",
    "unprov_base_rate = metric_sex.base_rate(privileged=False)\n",
    "\n",
    "print(\"For protected attribute: Sex\")\n",
    "print(f\"- Privileged Base Rate: {priv_base_rate}\")\n",
    "print(f\"- Unprivileged Base Rate: {unprov_base_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Calculating manuallyi with pandas\n",
    "# value_counts = df.groupby(\"Sex\")['Reincarcerated'].value_counts(normalize=True)\n",
    "# value_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the base rate for the privileged group\n",
    "value_counts.loc[0.0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the base rate for the unprivileged group\n",
    "value_counts.loc[1.0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the unpriviledged class for Sex is more likely to be a non-recividist (the desirable outcome)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Base Rate\n",
    "# print(explainer_train.base_rate())\n",
    "priv_base_rate = metric_race.base_rate()\n",
    "unprov_base_rate = metric_race.base_rate(privileged=False)\n",
    "\n",
    "print(\"For protected attribute: Race\")\n",
    "print(f\"- Privileged Base Rate: {priv_base_rate}\")\n",
    "print(f\"- Unprivileged Base Rate: {unprov_base_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The base_rate metric is useful for identifying disparities in outcomes across different groups and is often used in conjunction with other fairness metrics to provide a comprehensive assessment of fairness in machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Statistical Parity Difference\n",
    "\n",
    "Statistical parity difference measures the difference in the probability of favorable outcomes between unprivileged and privileged groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Parity Difference for Sex\n",
    "print(f\"Statistical Parity Difference (Sex): {metric_sex.statistical_parity_difference()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Statistical Parity Difference (Race): {metric_race.statistical_parity_difference()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the values are equivalent to the previous mean_difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Conclusion\n",
    "\n",
    "In this blog post, we explored how to create dataset objects using both the `BinaryLabelDataset` class in AI Fairness 360. We also discussed some key fairness metrics to evaluate bias in datasets and models. By leveraging these tools, you can take a significant step towards building fairer and more ethical AI systems. In the next blog post, we will explore using AI Fairness 360 to asses the fairness of a machine learning model and the intervetions we can take to alleviate the bias. \n",
    "\n",
    "\n",
    "### Further Reading\n",
    "- IBM AI Fairness 360: [AIF360 Documentation](https://aif360.mybluemix.net/)\n",
    "- Fairness and Machine Learning by Solon Barocas, Moritz Hardt, and Arvind Narayanan\n",
    "- Algorithmic Bias Detection and Mitigation: Best Practices and Policies to Reduce Consumer Harms by the Federal Trade Commission\n",
    "\n",
    "### Call to Action\n",
    "I encourage you to experiment with AI Fairness 360 in your projects and contribute to creating fair AI systems. Together, we can make AI work for everyone.\n",
    "\n",
    "### References\n",
    "- [Disparate Impact](https://en.wikipedia.org/wiki/Disparate_impact)\n",
    "- [AI Fairness 360 GitHub Repository](https://github.com/IBM/AIF360)\n",
    "- [Bias in AI: A Review of Literature](https://arxiv.org/abs/1908.09635)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "\n",
    "## Glossary for AI Fairness 360 Terms\n",
    "\n",
    "#### Protected Attributes\n",
    "**Definition**: Attributes that refer to characteristics of individuals that are legally protected against discrimination. These can include race, gender, age, religion, disability status, and other similar attributes.\n",
    "**Example**: In a dataset about job applicants, 'race' and 'gender' could be considered protected attributes.\n",
    "\n",
    "#### Privileged Groups\n",
    "**Definition**: Subsets of individuals within a dataset that have historically been favored or have had advantages in societal contexts based on certain protected attributes.\n",
    "**Example**: In the context of gender, males may be considered a privileged group in certain employment datasets.\n",
    "\n",
    "#### Unprivileged Groups\n",
    "**Definition**: Subsets of individuals within a dataset that have historically been disadvantaged or have faced discrimination based on certain protected attributes.\n",
    "**Example**: In the context of race, individuals identifying as Black or Hispanic might be considered unprivileged groups in certain societal contexts.\n",
    "\n",
    "#### Favorable Label\n",
    "**Definition**: The outcome or class in a dataset that is considered positive or beneficial for the individual.\n",
    "**Example**: In a dataset predicting loan approvals, a favorable label would be 'approved' (indicating that the loan application was successful).\n",
    "\n",
    "#### Unfavorable Label\n",
    "**Definition**: The outcome or class in a dataset that is considered negative or detrimental for the individual.\n",
    "**Example**: In a dataset predicting recidivism, an unfavorable label would be 'reoffended' (indicating that the individual reoffended after being released from prison).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# HOLD FOR BLOG POST 2:\n",
    "- Consistency\n",
    "- Equal Opportunity Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Equal Opportunity Difference\n",
    "\n",
    "Equal opportunity difference measures the difference in true positive rates between unprivileged and privileged groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "# Assuming you have a model's predictions for demonstration purposes\n",
    "# Example: true_labels and predicted_labels are arrays of true and predicted labels\n",
    "\n",
    "true_labels = binary_dataset.labels\n",
    "predicted_labels = binary_dataset.labels  # Placeholder, replace with actual model predictions\n",
    "\n",
    "# Create a ClassificationMetric object\n",
    "classification_metric = ClassificationMetric(\n",
    "    binary_dataset,\n",
    "    binary_dataset,  # Use the same dataset for demonstration\n",
    "    unprivileged_groups=[{'Race': 0}],\n",
    "    privileged_groups=[{'Race': 1}]\n",
    ")\n",
    "\n",
    "# Calculate Equal Opportunity Difference\n",
    "print(f\"Equal Opportunity Difference: {classification_metric.equal_opportunity_difference()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The consistency fairness metric measures how consistent the predictions of a machine learning model are for similar instances. It evaluates whether similar individuals receive similar predictions, which is an important aspect of fairness.\n",
    "\n",
    "To calculate the consistency metric, follow these steps:\n",
    "\n",
    "1. Identify Similar Instances: For each instance in the dataset, identify a set of similar instances. \n",
    "    - This can be done using a distance metric (e.g., Euclidean distance) in the feature space. \n",
    "    - The number of similar instances to consider is typically specified by a parameter like n_neighbors.\n",
    "\n",
    "2. Calculate Prediction Differences: For each instance, calculate the difference between its predicted outcome and the predicted outcomes of its similar instances.\n",
    "\n",
    "3. Average the Differences: Compute the average of these differences across all instances in the dataset.\n",
    "\n",
    "The formula for the consistency metric can be expressed as:\n",
    "\n",
    "$\\text{Consistency} = 1 - \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{k} \\sum_{j \\in \\text{Neighbors}(i)} | \\hat{y}_i - \\hat{y}_j | $\n",
    "\n",
    "Where:\n",
    "\n",
    "- $ N $ is the total number of instances.\n",
    "- $ k $ is the number of neighbors (similar instances) considered.\n",
    "- $ \\hat{y}_i$ is the predicted outcome for instance $ i $.\n",
    "- $ \\hat{y}_j $ is the predicted outcome for the $ j $-th neighbor of instance $ i $.\n",
    "\n",
    "\n",
    "> A higher consistency value indicates that the model's predictions are more consistent for similar instances, suggesting a fairer model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Consistency\n",
    "print(explainer_race.consistency())\n",
    "\n",
    "explainer_race.consistency(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Consistency\n",
    "print(explainer_sex.consistency())\n",
    "\n",
    "explainer_sex.consistency(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Add explanation about need for a model/predictions for final metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test/split with StandardDataset\n",
    "\n",
    "train, test = binary_dataset.split([0.7], shuffle=True)\n",
    "train.features.shape, test.features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=200,\n",
    "                             max_depth=12,\n",
    "                             min_samples_leaf=2,\n",
    "                             random_state=42, class_weight='balanced')\n",
    "clf.fit(train.features, train.labels.ravel())\n",
    "\n",
    "# Evaluate model\n",
    "y_hat_test = clf.predict(test.features)\n",
    "\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    \"\"\"Minimal evaluation of a model's performance.\"\"\"\n",
    "    print(classification_report(y_true.labels, y_pred))\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true.labels, y_pred,\n",
    "                                            normalize='true',cmap='Greens',\n",
    "                                            display_labels=['Non-Recid','Recid']);\n",
    "    \n",
    "evaluate_model(test, y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train), type(test), type(y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "# Create a ClassificationMetric object\n",
    "classification_metric = ClassificationMetric(test, test,\n",
    "                                             privileged_groups=privileged_groups,\n",
    "                                             unprivileged_groups=unprivileged_groups)\n",
    "                                             \n",
    "\n",
    "# Calculate consistency\n",
    "consistency_score = classification_metric.consistency(n_neighbors=5)\n",
    "print(f\"Consistency: {consistency_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Equal Opportunity Difference\n",
    "print(f\"Equal Opportunity Difference: {classification_metric.equal_opportunity_difference()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Statistical Parity Difference\n",
    "\n",
    "Statistical parity difference measures the difference in the probability of favorable outcomes between unprivileged and privileged groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Statistical Parity Difference for BinaryLabelDataset\n",
    "# print(f\"Statistical Parity Difference (BinaryLabelDataset): {binary_metric.statistical_parity_difference()}\")\n",
    "\n",
    "# Statistical Parity Difference for StandardDataset\n",
    "print(f\"Statistical Parity Difference (StandardDataset): {standard_metric.statistical_parity_difference()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Equal Opportunity Difference\n",
    "\n",
    "Equal opportunity difference measures the difference in true positive rates between unprivileged and privileged groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "# Assuming you have a model's predictions for demonstration purposes\n",
    "# Example: true_labels and predicted_labels are arrays of true and predicted labels\n",
    "\n",
    "true_labels = binary_dataset.labels\n",
    "predicted_labels = binary_dataset.labels  # Placeholder, replace with actual model predictions\n",
    "\n",
    "# Create a ClassificationMetric object\n",
    "classification_metric = ClassificationMetric(\n",
    "    binary_dataset,\n",
    "    binary_dataset,  # Use the same dataset for demonstration\n",
    "    unprivileged_groups=[{'race': 0}],\n",
    "    privileged_groups=[{'race': 1}]\n",
    ")\n",
    "\n",
    "# Calculate Equal Opportunity Difference\n",
    "print(f\"Equal Opportunity Difference: {classification_metric.equal_opportunity_difference()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Additional Similar Terms for AIF360\n",
    "\n",
    "#### Bias Mitigation\n",
    "**Definition**: The process of reducing or eliminating bias in machine learning models. This can involve techniques and algorithms that adjust the data or model to achieve fairer outcomes.\n",
    "**Example**: Reweighing, which adjusts the weights of different groups in the training data, is a bias mitigation technique.\n",
    "\n",
    "#### Disparate Impact\n",
    "**Definition**: A measure used to determine if a decision-making process disproportionately affects a particular protected group. It is calculated as the ratio of the rate of a favorable outcome for the unprivileged group to the rate of a favorable outcome for the privileged group.\n",
    "**Example**: If the hiring rate for women (unprivileged group) is 30% and for men (privileged group) is 60%, the disparate impact is 0.5 (30% / 60%).\n",
    "\n",
    "#### Fairness Metrics\n",
    "**Definition**: Quantitative measures used to assess the fairness of a machine learning model. These metrics evaluate how equally outcomes are distributed among different groups based on protected attributes.\n",
    "**Example**: Statistical parity, equal opportunity difference, and disparate impact are examples of fairness metrics.\n",
    "\n",
    "#### Bias Detection\n",
    "**Definition**: The process of identifying bias in datasets or machine learning models. This involves analyzing the data and model predictions to uncover patterns of unfair treatment based on protected attributes.\n",
    "**Example**: Calculating the disparate impact ratio to check if a model's predictions are biased against a particular racial group.\n",
    "\n",
    "#### Adversarial Debiasing\n",
    "**Definition**: A bias mitigation technique where an adversarial model is trained to predict the protected attribute from the original model’s predictions. The goal is to adjust the original model to make it difficult for the adversarial model to correctly predict the protected attribute, thus reducing bias.\n",
    "**Example**: Training an adversarial network alongside the main model to ensure that the predictions are not correlated with the protected attributes.\n",
    "\n",
    "---\n",
    "\n",
    "This glossary should help your readers understand the key concepts and terminology associated with AI Fairness 360. If you have any more terms to include or need further details, let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. **Assumptions for Binary Labels**:\n",
    "   - **Favorable and Unfavorable Classes**: `BinaryLabelDataset` explicitly handles the designation of favorable and unfavorable classes, streamlining tasks where such distinctions are important.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     dataset = BinaryLabelDataset(df, label_name='income', favorable_classes=[1], protected_attribute_names=['race'])\n",
    "     ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### StandardDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Purpose**: A more general-purpose dataset class that can handle a wide variety of data structures, including both binary and multi-class classification tasks.\n",
    "\n",
    "**Key Features**:\n",
    "- **Label Handling**: Can handle binary, multi-class, and continuous labels.\n",
    "- **Flexibility**: More flexible in terms of the types of data it can represent.\n",
    "- **Protected Attributes**: Handles protected attributes for fairness analysis.\n",
    "- **Customizability**: Provides more options for customization, including handling multiple protected attributes and various types of labels.\n",
    "- **Examples**: Suitable for datasets like the COMPAS dataset (recidivism prediction), where the task could involve multiple classes or continuous labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from aif360.datasets import StandardDataset\n",
    "\n",
    "# Create a StandardDataset\n",
    "standard_dataset = StandardDataset(\n",
    "    final_df,\n",
    "    label_name=DATA_DICT['label_names'][0], # Recidivism is the label\n",
    "    favorable_classes=[0], # Non-recidivism is favorable\n",
    "    protected_attribute_names=DATA_DICT['protected_attributes_names'], #['Race','Sex'],  # List of the protected attributes\n",
    "    privileged_classes=DATA_DICT[\"privileged_protected_attributes\"],#[[race_map['White']],[sex_map['Male']]]  # Privileged group: White\n",
    "    # unprivileged_classes = DATA_DICT[\"unprivileged_protected_attributes\"]\n",
    ")\n",
    "\n",
    "# Print dataset details\n",
    "print(standard_dataset.feature_names)\n",
    "print(standard_dataset.label_names)\n",
    "print(standard_dataset.protected_attribute_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "\n",
    "sens_ind = 0\n",
    "metric = BinaryLabelDatasetMetric(standard_dataset,\n",
    "                                  privileged_groups=[privileged_groups[sens_ind]],\n",
    "                                  unprivileged_groups=[unprivileged_groups[sens_ind]]\n",
    "                                  )\n",
    "\n",
    "print(f\"Disparate Impact: {metric.disparate_impact()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from aif360.datasets import StandardDataset\n",
    "\n",
    "# # Create a StandardDataset\n",
    "# standard_dataset = StandardDataset(\n",
    "#     final_df,\n",
    "#     label_name='Reincarcerated', # Recidivism is the label\n",
    "#     favorable_classes=[0], # Non-recidivism is favorable\n",
    "#     protected_attribute_names=['Race','Sex'],  # List of the protected attributes\n",
    "#     privileged_classes=[[race_map['White']],[sex_map['Male']]]  # Privileged group: White\n",
    "# )\n",
    "\n",
    "# # Print dataset details\n",
    "# print(standard_dataset.feature_names)\n",
    "# print(standard_dataset.label_names)\n",
    "# print(standard_dataset.protected_attribute_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### Comparison of BinaryLabel vs. Standard Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BinaryLabelDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Example Usage**:\n",
    "```python\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "dataset = BinaryLabelDataset(df, label_name='income', favorable_classes=[1], protected_attribute_names=['race'])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Advantages**:\n",
    "- Simplified handling of binary labels.\n",
    "- Directly compatible with binary classification metrics in AIF360.\n",
    "\n",
    "**Limitations**:\n",
    "- Limited to binary classification tasks.\n",
    "- Requires preprocessing if the dataset has multi-class labels or other complexities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StandardDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Example Usage**:\n",
    "```python\n",
    "from aif360.datasets import StandardDataset\n",
    "\n",
    "dataset = StandardDataset(df, label_name='recidivism', favorable_classes=[0], protected_attribute_names=['race'], privileged_classes=[[1]])\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- Greater flexibility and customizability.\n",
    "- Suitable for a broader range of tasks beyond binary classification, including multi-class and regression tasks.\n",
    "- Allows for more complex preprocessing and handling of different types of labels.\n",
    "\n",
    "**Limitations**:\n",
    "- More complex to set up and use compared to `BinaryLabelDataset`.\n",
    "- May require additional customization and configuration for specific use cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Key Differences\n",
    "\n",
    "1. **Target Use Case**:\n",
    "   - `BinaryLabelDataset`: Specifically designed for binary classification tasks.\n",
    "   - `StandardDataset`: General-purpose, suitable for binary, multi-class, and continuous label tasks.\n",
    "\n",
    "2. **Label Handling**:\n",
    "   - `BinaryLabelDataset`: Assumes binary labels.\n",
    "   - `StandardDataset`: Can handle binary, multi-class, and continuous labels.\n",
    "\n",
    "3. **Flexibility**:\n",
    "   - `BinaryLabelDataset`: Less flexible, optimized for simplicity in binary classification.\n",
    "   - `StandardDataset`: More flexible, can handle complex datasets with various types of labels and multiple protected attributes.\n",
    "\n",
    "4. **Setup and Configuration**:\n",
    "   - `BinaryLabelDataset`: Easier and quicker to set up for binary classification tasks.\n",
    "   - `StandardDataset`: Requires more setup and customization but offers greater versatility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### ~~Conclusion~~\n",
    "\n",
    "The choice between `BinaryLabelDataset` and `StandardDataset` depends on the specific requirements of your project. If you are working on a straightforward binary classification task, `BinaryLabelDataset` provides a simpler and more streamlined option. However, if your project involves more complex data structures, such as multi-class labels or multiple protected attributes, `StandardDataset` offers the flexibility and customizability needed to handle these complexities effectively.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import StandardDataset, BinaryLabelDataset\n",
    "\n",
    "# Assuming 'label' is your target column and 'gender' is your protected attribute\n",
    "\n",
    "binary_dataset = BinaryLabelDataset(\n",
    "        df=final_df,\n",
    "        label_names=[\"Reincarcerated\"],\n",
    "        protected_attribute_names = protected_attributes,\n",
    "        # privileged_groups = privileged_groups,\n",
    "        # favorable_label=[{\"Race\":0],\n",
    "        # unfavorable_label=1\n",
    "    )\n",
    "binary_dataset\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "# Create a BinaryLabelDataset with multiple protected attributes\n",
    "binary_dataset = BinaryLabelDataset(\n",
    "    df,\n",
    "    label_name='recidivism',\n",
    "    favorable_classes=[0],  # Non-recidivism is favorable\n",
    "    protected_attribute_names=['race', 'gender'],\n",
    "    privileged_classes=[[1], [1]]  # Privileged groups: White and Male\n",
    ")\n",
    "\n",
    "# Print dataset details\n",
    "print(binary_dataset.feature_names)\n",
    "print(binary_dataset.label_names)\n",
    "print(binary_dataset.protected_attribute_names)\n",
    "\n",
    "# target_col = \"Reincarcerated\"\n",
    "# # Creating a data dictionary like the one used in AIF360's medical expenditure example\n",
    "# data_dict={'feature_names':df.drop(coluns=target_col).columns,  # Question: Use transofrmed or original??\n",
    "#            'label_names':[target_col], \n",
    "#            'protected_attribute_names': [\"Sex\"],\n",
    "#            'privileged_protected_attributes': [np.array([1])], \n",
    "#            'unprivileged_protected_attributes': [np.array([0])],\n",
    "#            }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BinaryLabelDataset` class in AI Fairness 360 is a specialized subclass of `StandardDataset` designed specifically for binary classification tasks. Because `BinaryLabelDataset` is built on top of `StandardDataset`, it inherits all its functionalities and adds some specific conveniences for handling binary labels. Therefore, there is nothing inherently unique that `BinaryLabelDataset` can do that `StandardDataset` cannot. However, `BinaryLabelDataset` provides some convenience features and simplified handling specifically for binary classification tasks. Here’s a detailed look:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Capabilities of `StandardDataset`\n",
    "\n",
    "`StandardDataset` is more general and can handle a variety of data structures, including:\n",
    "\n",
    "1. **Multi-Class Classification**:\n",
    "   - **Multiple Labels**: `StandardDataset` can be used for tasks involving more than two classes, offering greater flexibility for datasets with multi-class labels.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     from aif360.datasets import StandardDataset\n",
    "\n",
    "     dataset = StandardDataset(df, label_name='health_status', favorable_classes=['healthy'], protected_attribute_names=['race'])\n",
    "     ```\n",
    "\n",
    "2. **Continuous Labels**:\n",
    "   - **Regression Tasks**: `StandardDataset` can handle datasets where the target variable is continuous, making it suitable for regression tasks.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     dataset = StandardDataset(df, label_name='income', favorable_classes=[lambda x: x > 50000], protected_attribute_names=['race'])\n",
    "     ```\n",
    "\n",
    "3. **Custom Preprocessing**:\n",
    "   - **Flexible Configuration**: `StandardDataset` allows for more complex preprocessing and configuration, accommodating a wide range of use cases beyond binary classification.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "While `BinaryLabelDataset` offers a streamlined, convenient interface specifically for binary classification tasks, anything it can do can also be achieved with `StandardDataset` through additional customization. The main advantage of using `BinaryLabelDataset` lies in its ease of use and the fact that it simplifies certain aspects of handling binary labels, such as setting up fairness metrics and defining favorable/unfavorable classes. However, for more complex tasks or different types of labels, `StandardDataset` is the more versatile and flexible option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've successfuly reproduced the structure of the original data dictionary, we have the information we need to follow the tutorial's example code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sex_map)\n",
    "# [{\"Sex\":v} for k,v in sex_map.items() if not k.lower().startswith('m')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a detailed comparison and contrast of these two classes.\n",
    "\n",
    "**BinaryLabelDataset**\n",
    "\n",
    "Purpose: Designed specifically for binary classification tasks where the target variable (label) has only two possible outcomes.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "* Label Handling: Assumes that the label (target variable) has binary values (e.g., 0 and 1, “yes” and “no”).\n",
    "* Protected Attributes: Handles protected attributes for fairness analysis.\n",
    "* Metrics Compatibility: Provides compatibility with fairness metrics that are specific to binary classification tasks.\n",
    "* Examples: Suitable for datasets like the Adult Income dataset, where the task is to predict if income is above or below $50K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Creating a `StandardDataset`\n",
    "\n",
    "The `StandardDataset` is more flexible and can handle binary, multi-class, and continuous labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE ON UPDATING data_dict and unprivileged_groups/privileged_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- I think data_dict needs to have...\n",
    ">    - Just numbers in arrays?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOOKMARK: CALL WITH BRENDA 07/25/24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### BOOKMARK: CALL WITH BRENDA 07/25/24\n",
    "# ### alt way to create priv and unpriv groups\n",
    "# prot_attrs = ['Race','Sex']\n",
    "# privileged_protected_attributes = [[0],[0]]\n",
    "\n",
    "# protected_attr_maps = {\"Race\":race_map,\n",
    "#                  \"Sex\": sex_map}\n",
    "\n",
    "# # unprivileged_protected_attributes = []/\n",
    "# unprivileged_groups = []\n",
    "# for idx, protected_attr in enumerate(prot_attrs):\n",
    "#     temp_unpriv = []\n",
    "#     for k,v in protected_attr_maps[protected_attr].items():\n",
    "#         if v not in privileged_protected_attributes[idx]:#[0]:\n",
    "#             temp_unpriv.append(v)\n",
    "#         # privileged_protected_attributes.append(v_)\n",
    "#         # else:\n",
    "#     if len(temp_unpriv) > 0:\n",
    "#         unprivileged_groups.append( np.array(temp_unpriv)\n",
    "#         # unprivileged_protected_attributes.append({protected_attr:temp_unpriv})\n",
    "#         # unprivileged_protected_attributes.append({protected_attr:v})\n",
    "# unprivileged_groups\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# pprint(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # # Define the unprivileged and privileged groups\n",
    "# privileged_groups =[ ]\n",
    "# unprivileged_groups = [ ]\n",
    "\n",
    "# for protected_attr in data_dict['privileged_protected_attributes']:\n",
    "#     privileged_groups.append(protected_attr)\n",
    "# for protected_attr in data_dict['unprivileged_protected_attributes']:\n",
    "#     unprivileged_groups.append(protected_attr)\n",
    "\n",
    "# privileged_groups, unprivileged_groups\n",
    "\n",
    "# # for protected_attr in data_dict['protected_attribute_names']:\n",
    "    \n",
    "# #     temp = data_dict['privileged_protected_attributes']\n",
    "# #     privileged_groups.append({protected_attr: temp[0][0]})\n",
    "    \n",
    "# #     temp = data_dict['unprivileged_protected_attributes']\n",
    "# #     unprivileged_groups.append({protected_attr: temp[0][0]})\n",
    "# # privileged_groups, unprivileged_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Exploring Fairness Metrics\n",
    "\n",
    "AI Fairness 360 provides several fairness metrics to evaluate bias in datasets and models. We'll explore some key metrics using both `BinaryLabelDataset` and `StandardDataset`.\n",
    "\n",
    "#### Disparate Impact\n",
    "\n",
    "Disparate impact is a ratio that measures the relative likelihood of a favorable outcome between unprivileged and privileged groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.explainers import MetricTextExplainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Disparate Impact for BinaryLabelDataset\n",
    "# binary_metric = BinaryLabelDatasetMetric(\n",
    "#     binary_dataset,\n",
    "#     privileged_groups=[{'Race': 0}],\n",
    "#     unprivileged_groups=[{\"Race\":i} for i in range(1,len(race_map))]\n",
    "# )\n",
    "# print(f\"Disparate Impact (BinaryLabelDataset): {binary_metric.disparate_impact()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# priv_set = set(privileged_groups)\n",
    "# unpriv_set = set(unprivileged_groups)\n",
    "# priv_set.disjoint(unpriv_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Disparate Impact for StandardDataset\n",
    "standard_metric_race = BinaryLabelDatasetMetric(\n",
    "    standard_dataset,\n",
    "    privileged_groups=privileged_groups,#[{'Race': race_map['White']}], # White\n",
    "    unprivileged_groups=unprivileged_groups#[{\"Race\":i} for race,i in race_map.items() if \"White\" not in race] # All other\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Creating a `BinaryLabelDataset`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate ClassificationMetrics we need to use a special variant of the StandardDataset - the BinaryLabel Dataset.\n",
    "`TO DO: ADD BRIEF EXPLAINATION RE: DIFFERENCE BETWEEN STANDARD AND BINARY LABEL DATASETS`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `BinaryLabelDataset` is tailored for binary classification tasks where the target variable has two possible outcomes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "# Create a BinaryLabelDataset\n",
    "binary_dataset = BinaryLabelDataset(\n",
    "    df = final_df,\n",
    "    label_names=['Reincarcerated'],\n",
    "    # favorable_classes=[0],  # Non-recidivism is favorable\n",
    "    favorable_label=0,  # Non-recidivism is favorable\n",
    "    unfavorable_label=1, # Recidivism is unfavorable\n",
    "    protected_attribute_names=['Race','Sex'],\n",
    "    # privileged_classes=[[0],[0]]  # Privileged group: White\n",
    ")\n",
    "\n",
    "# Print dataset details\n",
    "print(binary_dataset.feature_names)\n",
    "print(binary_dataset.label_names)\n",
    "print(binary_dataset.protected_attribute_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO: Need to add a model for the metrics below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Adding Multiple Protected Attributes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here’s how you can specify multiple protected attributes for both `BinaryLabelDataset` and `StandardDataset`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Example with `BinaryLabelDataset`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's say you want to include both `race` and `gender` as protected attributes:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "# Encode 'race' and 'gender'\n",
    "df['race'] = df['race'].map({'Black': 0, 'White': 1, 'Other': 2})\n",
    "df['gender'] = df['gender'].map({'Male': 1, 'Female': 0})\n",
    "\n",
    "# Create a BinaryLabelDataset with multiple protected attributes\n",
    "binary_dataset = BinaryLabelDataset(\n",
    "    df,\n",
    "    label_name='recidivism',\n",
    "    favorable_classes=[0],  # Non-recidivism is favorable\n",
    "    protected_attribute_names=['race', 'gender'],\n",
    "    privileged_classes=[[1], [1]]  # Privileged groups: White and Male\n",
    ")\n",
    "\n",
    "# Print dataset details\n",
    "print(binary_dataset.feature_names)\n",
    "print(binary_dataset.label_names)\n",
    "print(binary_dataset.protected_attribute_names)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Example with `StandardDataset`\n",
    "\n",
    "Similarly, for `StandardDataset`, you can specify multiple protected attributes:\n",
    "\n",
    "```python\n",
    "from aif360.datasets import StandardDataset\n",
    "\n",
    "# Create a StandardDataset with multiple protected attributes\n",
    "standard_dataset = StandardDataset(\n",
    "    df,\n",
    "    label_name='recidivism',\n",
    "    favorable_classes=[0],\n",
    "    protected_attribute_names=['race', 'gender'],\n",
    "    privileged_classes=[[1], [1]]  # Privileged groups: White and Male\n",
    ")\n",
    "\n",
    "# Print dataset details\n",
    "print(standard_dataset.feature_names)\n",
    "print(standard_dataset.label_names)\n",
    "print(standard_dataset.protected_attribute_names)\n",
    "```\n",
    "\n",
    "### Exploring Fairness Metrics with Multiple Protected Attributes\n",
    "\n",
    "When using multiple protected attributes, you can still calculate fairness metrics. Here’s how you can do it:\n",
    "\n",
    "#### Disparate Impact\n",
    "\n",
    "Calculate disparate impact considering multiple protected attributes:\n",
    "\n",
    "```python\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "\n",
    "# Disparate Impact for BinaryLabelDataset\n",
    "binary_metric = BinaryLabelDatasetMetric(\n",
    "    binary_dataset,\n",
    "    privileged_groups=[{'race': 1, 'gender': 1}],\n",
    "    unprivileged_groups=[{'race': 0, 'gender': 0}]\n",
    ")\n",
    "print(f\"Disparate Impact (BinaryLabelDataset): {binary_metric.disparate_impact()}\")\n",
    "\n",
    "# Disparate Impact for StandardDataset\n",
    "standard_metric = BinaryLabelDatasetMetric(\n",
    "    standard_dataset,\n",
    "    privileged_groups=[{'race': 1, 'gender': 1}],\n",
    "    unprivileged_groups=[{'race': 0, 'gender': 0}]\n",
    ")\n",
    "print(f\"Disparate Impact (StandardDataset): {standard_metric.disparate_impact()}\")\n",
    "```\n",
    "\n",
    "#### Statistical Parity Difference\n",
    "\n",
    "Calculate statistical parity difference considering multiple protected attributes:\n",
    "\n",
    "```python\n",
    "# Statistical Parity Difference for BinaryLabelDataset\n",
    "print(f\"Statistical Parity Difference (BinaryLabelDataset): {binary_metric.statistical_parity_difference()}\")\n",
    "\n",
    "# Statistical Parity Difference for StandardDataset\n",
    "print(f\"Statistical Parity Difference (StandardDataset): {standard_metric.statistical_parity_difference()}\")\n",
    "```\n",
    "\n",
    "#### Equal Opportunity Difference\n",
    "\n",
    "Calculate equal opportunity difference considering multiple protected attributes:\n",
    "\n",
    "```python\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "# Assuming you have a model's predictions for demonstration purposes\n",
    "true_labels = binary_dataset.labels\n",
    "predicted_labels = binary_dataset.labels  # Placeholder, replace with actual model predictions\n",
    "\n",
    "# Create a ClassificationMetric object\n",
    "classification_metric = ClassificationMetric(\n",
    "    binary_dataset,\n",
    "    binary_dataset,  # Use the same dataset for demonstration\n",
    "    unprivileged_groups=[{'race': 0, 'gender': 0}],\n",
    "    privileged_groups=[{'race': 1, 'gender': 1}]\n",
    ")\n",
    "\n",
    "# Calculate Equal Opportunity Difference\n",
    "print(f\"Equal Opportunity Difference: {classification_metric.equal_opportunity_difference()}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Conclusion\n",
    "\n",
    "In summary, AI Fairness 360 allows you to handle multiple protected attributes in both `BinaryLabelDataset` and `StandardDataset`. This capability is crucial for analyzing and mitigating bias across various dimensions, ensuring a more comprehensive approach to fairness in AI systems.\n",
    "\n",
    "Feel free to expand on any sections or let me know if you need further details or specific explanations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLOG V1\n",
    "\n",
    "## Title: **AI Fairness 360: Ensuring Equity in Machine Learning**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 👉Defining the Protected Attributes and Priveledged/Unpriveldged Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the DataFrame to an AIF360-compatible dataset\n",
    "# protected_attributes= [ 'Race','Sex']\n",
    "# privileged_groups = [{'Race': 0}, {\"Sex\":0}]\n",
    "# unprivileged_groups = [{\"Race\":i} for i in range(1,len(race_map))] + [{\"Sex\":1}]\n",
    "# unprivileged_groups\n",
    "\n",
    "# # # Convert the DataFrame to an AIF360-compatible dataset\n",
    "# # protected_attributes= [ 'Sex']\n",
    "# # privileged_groups = [{\"Sex\":0}]\n",
    "# # unprivileged_groups = [{\"Sex\":1}]\n",
    "# # unprivileged_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the AIF360 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remaining categorical features must be encoded\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# encoder = OneHotEncoder(drop=None, sparse_output=False)\n",
    "\n",
    "# # Drop unnecessary columns \n",
    "# drop_cols = ['Supervising Unit','Supervision Start Date','Supervision End Date']\n",
    "\n",
    "# # Categorical columns to be encoded\n",
    "# cat_cols=  ['Supervision Type','Supervision End Reason',\"Supervision End Reason\",'Supervision Offense Class','Supervision Offense Type',\n",
    "#        'Supervision Offense Subtype']\n",
    "\n",
    "\n",
    "# ohe_df = encoder.fit_transform(df[cat_cols])\n",
    "\n",
    "# # Concatenate one-hot encoded columns with the original DataFrame\n",
    "# # final_df = pd.concat([df.drop(columns=[*cat_cols, *drop_cols]), ohe_df],axis=1)\n",
    "# final_df = pd.get_dummies(df.drop(columns=drop_cols), columns=cat_cols)\n",
    "# final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj_cols = final_df.select_dtypes('object').columns\n",
    "# obj_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df[obj_cols] = final_df[obj_cols].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the unprivileged and privileged groups\n",
    "# privileged_groups =[ ]\n",
    "# unprivileged_groups = [ ]\n",
    "\n",
    "# for protected_attr in data_dict['protected_attribute_names']:\n",
    "    \n",
    "#     temp = data_dict['privileged_protected_attributes']\n",
    "#     privileged_groups.append({protected_attr: temp[0][0]})\n",
    "    \n",
    "#     temp = data_dict['unprivileged_protected_attributes']\n",
    "#     unprivileged_groups.append({protected_attr: temp[0][0]})\n",
    "# privileged_groups, unprivileged_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW BLOG POST: Detecting and Mitigatin Bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NEED NEW INTRO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Exploring the Dataset**\n",
    "Before diving into bias detection, it's important to understand the dataset. The Adult Income dataset includes features such as age, education, occupation, race, and gender.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "print(dataset.features)\n",
    "print(dataset.labels)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Detecting Bias**\n",
    "Next, we will use AI Fairness 360 to detect bias in the dataset. We will start by calculating the disparate impact, a commonly used fairness metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "\n",
    "metric = BinaryLabelDatasetMetric(dataset, privileged_groups=[{'race': 1}], unprivileged_groups=[{'race': 0}])\n",
    "print(f\"Disparate Impact: {metric.disparate_impact()}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Mitigating Bias**\n",
    "After detecting bias, the next step is to mitigate it. We will use the reweighing algorithm to adjust the weights of different groups in the dataset to ensure fairer outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "\n",
    "rw = Reweighing(unprivileged_groups=[{'race': 0}], privileged_groups=[{'race': 1}])\n",
    "dataset_transf = rw.fit_transform(dataset)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Evaluating Results**\n",
    "Finally, we will evaluate the effectiveness of our bias mitigation efforts by comparing the disparate impact metric before and after applying the reweighing algorithm.\n",
    "\n",
    "```python\n",
    "metric_transf = BinaryLabelDatasetMetric(dataset_transf, privileged_groups=[{'race': 1}], unprivileged_groups=[{'race': 0}])\n",
    "print(f\"Disparate Impact after mitigation: {metric_transf.disparate_impact()}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Conclusion\n",
    "\n",
    "In this lesson, we explored the importance of fairness in AI and how the AI Fairness 360 toolkit can help detect and mitigate bias in machine learning models. By ensuring our AI systems operate fairly, we can create more equitable and just outcomes for all.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Further Reading\n",
    "- IBM AI Fairness 360: [AIF360 Documentation](https://aif360.mybluemix.net/)\n",
    "- Fairness and Machine Learning by Solon Barocas, Moritz Hardt, and Arvind Narayanan\n",
    "- Algorithmic Bias Detection and Mitigation: Best Practices and Policies to Reduce Consumer Harms by the Federal Trade Commission\n",
    "\n",
    "### Call to Action\n",
    "I encourage you to experiment with AI Fairness 360 in your projects and contribute to creating fair AI systems. Together, we can make AI work for everyone.\n",
    "\n",
    "### References\n",
    "- [Disparate Impact](https://en.wikipedia.org/wiki/Disparate_impact)\n",
    "- [AI Fairness 360 GitHub Repository](https://github.com/IBM/AIF360)\n",
    "- [Bias in AI: A Review of Literature](https://arxiv.org/abs/1908.09635)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old way of preparing data dict and groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recreateing the privileged_groups to match the tutorial example\n",
    "# # Make new mapping of group integers -> group names\n",
    "# inverse_map = {v:k for k,v in current_attr_map_dict.items()}\n",
    "\n",
    "# # Empoty List to save integer arrays of protected_attributes\n",
    "# current_priviledged_protected_attributes = []\n",
    "# current_unpriviledged_protected_attributes = []\n",
    " \n",
    "# for group_num in unique_groups:\n",
    "#     # Get the name of the current group\n",
    "#     group_name = inverse_map[group_num]\n",
    "    \n",
    "#     # If the group_name is in list of priviledged groups, append to the priviledged list\n",
    "#     if group_name in current_attr_priv_group_names:\n",
    "#         current_priviledged_protected_attributes.append(group_num)\n",
    "        \n",
    "#     # Else, append to the unprivileged list    \n",
    "#     else:\n",
    "#         current_unpriviledged_protected_attributes.append(group_num)\n",
    "\n",
    "# ## Convert the lists to numpy arrays\n",
    "# current_priviledged_protected_attributes = np.array(current_priviledged_protected_attributes)\n",
    "# current_unpriviledged_protected_attributes = np.array(current_unpriviledged_protected_attributes)\n",
    "# current_priviledged_protected_attributes, current_unpriviledged_protected_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# current_privileged_prot_attrs = [np.array(current_attr_map_dict['White'])]\n",
    "# current_unprivileged_prot_attrs = [{current_attr: v} for v in unique_groups if v not in current_privileged_groups]\n",
    "\n",
    "# current_privileged_protected_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the unprivileged_groups\n",
    "# current_unprivileged_groups = [{current_attr: v} for v in unique_groups if v not in current_privileged_groups]\n",
    "# current_privileged_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# data_dict={'feature_names':feature_names\n",
    "#            'label_names':label_names, \n",
    "#            'protected_attribute_names': [\"Race\",\"Sex\"],\n",
    "#            'privileged_protected_attributes': [{'Race':race_map['White']},{\"Sex\":sex_map['Male']}] , #[np.array([1])], \n",
    "#            'unprivileged_protected_attributes': [{\"Race\":v} for k,v in race_map.items() if \"white\" not in k.lower()] + \n",
    "#                [{\"Sex\":v} for k,v in sex_map.items() if not k.lower().startswith('m')]\n",
    "                \n",
    "#            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the unprivileged and privileged groups\n",
    "# privileged_groups =[ ]\n",
    "# unprivileged_groups = [ ]\n",
    "\n",
    "# # For eeach protected attribute, define the privileged and unprivileged groups\n",
    "# for protected_attr in data_dict['protected_attribute_names']:\n",
    "    \n",
    "#     temp = data_dict['privileged_protected_attributes']\n",
    "#     privileged_groups.append({protected_attr: temp[0][0]})\n",
    "    \n",
    "#     temp = data_dict['unprivileged_protected_attributes']\n",
    "#     unprivileged_groups.append({protected_attr: temp[0][0]})\n",
    "# privileged_groups, unprivileged_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fair-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
